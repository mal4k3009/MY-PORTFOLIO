<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Explore the projects of Malak J Bhadgaonkar, including the Sign2Speak project.">
    <title>Projects - Malak J Bhadgaonkar</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="about.html">About</a></li>
                <li><a href="projects.html">Projects</a></li>
                <li><a href="skills.html">Skills</a></li>
                <li><a href="contact.html">Contact</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <h1>My Projects</h1>

        <section>
            <h2>Sign2Speak: Converting Sign Language to Speech and Text</h2>
            <p><strong>Description:</strong> Sign2Speak is a project aimed at bridging the communication gap between individuals who use sign language and those who do not. By leveraging computer vision and machine learning technologies, this project translates sign language gestures into both text and spoken words in real-time.</p>

            <p><strong>Technologies Used:</strong> 
                <ul>
                    <li>YOLOv5 (for real-time object detection)</li>
                    <li>Python</li>
                    <li>OpenCV (for video processing)</li>
                    <li>Tesseract OCR (for text recognition)</li>
                    <li>Text-to-Speech (TTS) engine</li>
                </ul>
            </p>

            <p><strong>Key Features:</strong>
                <ul>
                    <li>Real-time recognition of hand gestures.</li>
                    <li>Conversion of sign language into readable text.</li>
                    <li>Speech output generated from text to make communication more inclusive.</li>
                </ul>
            </p>

            <p><strong>Challenges and Learning:</strong> 
                <ul>
                    <li>One of the major challenges was achieving accurate real-time recognition of gestures, particularly when dealing with varying lighting conditions or background noise. By optimizing the model and tuning the detection parameters, I was able to significantly improve the accuracy and speed.</li>
                    <li>Another challenge was synchronizing the text and speech output to ensure smooth and seamless communication. This required fine-tuning the processing pipeline to minimize delays between recognition and output.</li>
                    <li>Through this project, I learned a great deal about the complexities of human-computer interaction, the power of computer vision in solving accessibility issues, and the potential of AI-driven solutions for real-world applications.</li>
                </ul>
            </p>

            <p><strong>Status:</strong> The project is currently a functional prototype, and I plan to continue refining it to improve accuracy, add more gestures, and potentially deploy it as a standalone application in the future.</p>

            <p><strong>Project Link:</strong> (Link not available yet)</p>
        </section>

    </main>
    <footer>
        <p>&copy; 2024 Malak J Bhadgaonkar</p>
    </footer>
</body>
</html>
